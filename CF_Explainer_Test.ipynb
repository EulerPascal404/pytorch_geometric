{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd495b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "#from torch_geometric.explain import CFExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "# Import other necessary modules as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40355f86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install jupyterthemes\n",
    "# !jt -t chesterish\n",
    "#!jt -r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb5791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.nn import GCNConv, Set2Set\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead1edc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e75b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a0db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = os.path.join(os.getcwd(), 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, dataset)\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3881c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(data.x, data.edge_index)[83])\n",
    "data.y[83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35684884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(3)*torch.tensor(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec1f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 'conv1.bias': requires_grad=True\n",
      "Parameter 'conv1.lin.weight': requires_grad=True\n",
      "Parameter 'conv2.bias': requires_grad=True\n",
      "Parameter 'conv2.lin.weight': requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter '{name}': requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21fb3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch_geometric.explain import ExplainerConfig, Explanation, ModelConfig\n",
    "from torch_geometric.explain.algorithm import ExplainerAlgorithm\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import MaskType, ModelMode, ModelTaskLevel\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "#from torch_geometric.utils import dense_adjacency\n",
    "\n",
    "\n",
    "class CFExplainer(ExplainerAlgorithm):\n",
    "    r\"\"\"The CF-Explainer model from the `\"CF-GNNExplainer: Counterfactual Explanations for Graph Neural\n",
    "Networks\"\n",
    "    <https://arxiv.org/abs/2102.03322>`_ paper for generating CF explanations for GNNs: \n",
    "    the minimal perturbation to the input (graph) data such that the prediction changes.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using :class:`GNNExplainer`, see\n",
    "        `examples/explain/gnn_explainer.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer.py>`_,\n",
    "        `examples/explain/gnn_explainer_ba_shapes.py <https://github.com/\n",
    "        pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        explain/gnn_explainer_ba_shapes.py>`_, and `examples/explain/\n",
    "        gnn_explainer_link_pred.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer_link_pred.py>`_.\n",
    "\n",
    "    Args:\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        **kwargs (optional): Additional hyper-parameters to override default\n",
    "            settings in\n",
    "            :attr:`~torch_geometric.explain.algorithm.GNNExplainer.coeffs`.\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'beta' : .001,\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "        'EPS': 1e-15,\n",
    "    }\n",
    "\n",
    "    def __init__(self, epochs: int = 100, lr: float = 0.01, cf_optimizer = \"SGD\", n_momentum = 0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.cf_optimizer = cf_optimizer\n",
    "        self.n_momentum = n_momentum\n",
    "        self.coeffs.update(kwargs)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n",
    "        self.best_cf_example = None\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        index: int = None,\n",
    "        **kwargs,\n",
    "    ) -> Explanation:\n",
    "        if isinstance(x, dict) or isinstance(edge_index, dict):\n",
    "            raise ValueError(f\"Heterogeneous graphs not yet supported in \"\n",
    "                             f\"'{self.__class__.__name__}'\")\n",
    "\n",
    "        self._train(model, x, edge_index, index=index, **kwargs)\n",
    "\n",
    "        # node_mask = self._post_process_mask(\n",
    "        #     self.best_cf_example[0],\n",
    "        #     self.hard_node_mask,\n",
    "        #     apply_sigmoid=True,\n",
    "        # )\n",
    "        node_mask = self._post_process_mask(\n",
    "            self.node_mask,\n",
    "            self.hard_node_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "        edge_mask = self._post_process_mask(\n",
    "            self.edge_mask,\n",
    "            self.hard_edge_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "\n",
    "        self._clean_model(model)\n",
    "\n",
    "        return Explanation(node_mask=node_mask, edge_mask=edge_mask)\n",
    "\n",
    "    def supports(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: int = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._initialize_masks(x, edge_index)\n",
    "        parameters = []\n",
    "        #if self.node_mask is not None:\n",
    "        #    parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "    \n",
    "        if self.cf_optimizer == \"SGD\" and self.n_momentum == 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr)\n",
    "        elif self.cf_optimizer == \"SGD\" and self.n_momentum != 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr, nesterov=True, momentum=n_momentum)\n",
    "        elif self.cf_optimizer == \"Adadelta\":\n",
    "            optimizer = torch.optim.Adadelta(parameters, lr=self.lr)\n",
    "        else:\n",
    "            raise Exception(\"Optimizer is not currently supported.\")\n",
    "        \n",
    "        original_prediction  = model(x, edge_index, **kwargs)\n",
    "        print('org pred', original_prediction[index])\n",
    "        print('target', target[index])\n",
    "        for i in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            h = x # if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            #discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask)>=0.5, 1, 0)\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            y_hat, y = model(h, edge_index, **kwargs), target\n",
    "            \n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss = self._loss(y_hat, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "                print(\"self.hard_edge_mask\", torch.sum(self.hard_edge_mask))\n",
    "\n",
    "    def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "        node_mask_type = None\n",
    "        edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "        device = x.device\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        if node_mask_type is None:\n",
    "            self.node_mask = None\n",
    "        elif node_mask_type == MaskType.object:\n",
    "            self.node_mask = Parameter(torch.ones(N, 1, device=device))\n",
    "        elif node_mask_type == MaskType.attributes:\n",
    "            self.node_mask = Parameter(torch.ones(N, F, device=device))\n",
    "        elif node_mask_type == MaskType.common_attributes:\n",
    "            self.node_mask = Parameter(torch.ones(1, F, device=device))\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if edge_mask_type is None:\n",
    "            self.edge_mask = None\n",
    "        elif edge_mask_type == MaskType.object:\n",
    "            self.edge_mask = Parameter(torch.ones(E, device=device))\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "\n",
    "\n",
    "    def _loss(self, y_hat: Tensor, y: Tensor) -> Tensor:\n",
    "        print('y_hat', y_hat)\n",
    "#        # Calculate the sigmoid of self.edge_mask\n",
    "#         edge_mask_sigmoid = torch.sigmoid(self.edge_mask)\n",
    "\n",
    "#         # Calculate the absolute difference between 1.0 and edge_mask_sigmoid\n",
    "#         edge_mask_penalty = torch.abs(1.0 - edge_mask_sigmoid)\n",
    "\n",
    "#         # Threshold edge_mask_penalty elements at 0.5 using element-wise multiplication\n",
    "#         threshold = 0.5\n",
    "#         edge_mask_penalty = edge_mask_penalty * (edge_mask_sigmoid >= threshold).float()\n",
    "\n",
    "#         # Calculate the negative log-likelihood loss\n",
    "#         log_probs = torch.log(edge_mask_sigmoid)\n",
    "#         loss_nll = -log_probs.mean()\n",
    "\n",
    "#         # Additional term: penalize similarity between edge_mask_sigmoid and 1.0\n",
    "#         separation_term = torch.abs(edge_mask_sigmoid - 1.0)\n",
    "\n",
    "#         # Combine all loss terms\n",
    "#         loss = loss_nll + separation_term.mean() + edge_mask_penalty.mean()\n",
    "\n",
    "        if self.model_config.mode == ModelMode.binary_classification:\n",
    "            loss = self._loss_binary_classification(y_hat, y)\n",
    "        elif self.model_config.mode == ModelMode.multiclass_classification:\n",
    "            loss = self._loss_multiclass_classification(y_hat, y)\n",
    "        elif self.model_config.mode == ModelMode.regression:\n",
    "            loss = self._loss_regression(y_hat, y)\n",
    "        else:\n",
    "            assert False\n",
    "        if self.hard_edge_mask is not None:\n",
    "            assert self.edge_mask is not None\n",
    "            m = self.edge_mask[self.hard_edge_mask].sigmoid()\n",
    "            edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "            loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "            ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "                1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "            loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "#         if self.hard_node_mask is not None:\n",
    "#             assert self.node_mask is not None\n",
    "#             m = self.node_mask[self.hard_node_mask].sigmoid()\n",
    "#             node_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "#             loss = loss + self.coeffs['node_feat_size'] * node_reduce(m)\n",
    "#             ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "#                 1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "#             loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "        print(loss)\n",
    "        return loss\n",
    "#     def _loss(self, y_hat: Tensor, y: Tensor, edge_index, index) -> Tensor:\n",
    "#         if self.model_config.mode == ModelMode.binary_classification:\n",
    "#             loss = -1 * self._loss_binary_classification(y_hat, y)\n",
    "#         elif self.model_config.mode == ModelMode.multiclass_classification:\n",
    "#             loss = -1 * self._loss_multiclass_classification(y_hat, y)\n",
    "#         elif self.model_config.mode == ModelMode.regression:\n",
    "#             loss = -1 * self._loss_regression(y_hat, y)\n",
    "#         else:\n",
    "#             assert False\n",
    "#         #         y_hat_discrete = torch.argmax(y_hat, dim=-1)  # Compute argmax along the class dimension\n",
    "# #         y_discrete = torch.argmax(y, dim=-1)  # Compute argmax along the class dimension\n",
    "        \n",
    "# #         print(\"y_hat.requires_grad:\", y_hat.requires_grad)\n",
    "# #         print(\"y.requires_grad:\", y.requires_grad)\n",
    "# #         print(\"edge_index.requires_grad:\", edge_index.requires_grad)\n",
    "# #         #pred_same_indicator = torch.where(y_hat_discrete == y_discrete, torch.tensor(-1.0), torch.tensor(-1.0))\n",
    "# #         loss_pred = - F.nll_loss(F.log_softmax(y_hat, dim=-1), y_discrete.long(), reduction='none')\n",
    "        \n",
    "# #         print(\"loss_pred.requires_grad:\", loss_pred.requires_grad)\n",
    "# #         print(\"self.edge_mask.requires_grad:\", self.edge_mask.requires_grad)\n",
    "# #         #loss_pred = F.nll_loss(F.log_softmax(y_hat, dim=-1), y_discrete.long(), reduction)\n",
    "# # #         if self.model_config.mode == ModelMode.binary_classification:\n",
    "# # #             loss_pred = self._loss_binary_classification(y_hat, y.long())\n",
    "# # #         elif self.model_config.mode == ModelMode.multiclass_classification:\n",
    "# # #             loss_pred = self._loss_multiclass_classification(y_hat, y.long())\n",
    "# # # #         elif self.model_config.mode == ModelMode.regression:\n",
    "# # # #             loss_pred = - self._loss_regression(y_hat, y)\n",
    "# # #         else:\n",
    "# # #             assert False\n",
    "# #         print(\"self.edge mask:\", self.edge_mask)\n",
    "# #         discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask) > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
    "# #         print(\"discrete_edge_mask.requires_grad:\", discrete_edge_mask.requires_grad)\n",
    "# #         discrete_edge_mask.requires_grad = False\n",
    "# #         print(\"discrete_edge_mask.requires_grad:\", discrete_edge_mask.requires_grad)\n",
    "# #         print(\"discrete edge mask\", discrete_edge_mask)\n",
    "# #         new_edge_index = edge_index * discrete_edge_mask\n",
    "\n",
    "# #         loss_graph_dist = torch.sum(torch.abs(new_edge_index - edge_index)) / 2\n",
    "# #         #print(\"loss_graph_dist\", loss_graph_dist)\n",
    "# #         loss_total = loss_pred\n",
    "# #         #loss_total = pred_same_indicator * loss_pred + self.coeffs['beta'] * loss_graph_dist\n",
    "# #         print(\"loss_total\", loss_total)\n",
    "#         loss_total = loss\n",
    "#         print(loss_total)\n",
    "#         return loss_total\n",
    "\n",
    "    def _clean_model(self, model):\n",
    "        clear_masks(model)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f730c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb6ad07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value tensor(2)\n",
      "org pred tensor([-7.0778, -5.5675, -0.0099, -6.1459, -7.2232, -6.2567, -7.8279],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "target tensor(2)\n",
      "y_hat tensor([-7.0778, -5.5675, -0.0099, -6.1459, -7.2232, -6.2567, -7.8279],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.0099, grad_fn=<NllLossBackward0>)\n",
      "self.hard_edge_mask tensor(8)\n",
      "y_hat tensor([-7.0779, -5.5675, -0.0099, -6.1460, -7.2233, -6.2567, -7.8280],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6213, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0785, -5.5680, -0.0099, -6.1465, -7.2239, -6.2573, -7.8287],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6213, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0791, -5.5684, -0.0099, -6.1470, -7.2245, -6.2578, -7.8294],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6212, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0797, -5.5688, -0.0099, -6.1475, -7.2251, -6.2583, -7.8301],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6212, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0803, -5.5693, -0.0099, -6.1480, -7.2257, -6.2588, -7.8308],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6211, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0809, -5.5697, -0.0099, -6.1485, -7.2262, -6.2593, -7.8315],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6211, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0815, -5.5701, -0.0099, -6.1490, -7.2268, -6.2598, -7.8322],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6210, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0821, -5.5706, -0.0099, -6.1496, -7.2274, -6.2603, -7.8329],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6210, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0827, -5.5710, -0.0099, -6.1501, -7.2280, -6.2609, -7.8336],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6209, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0833, -5.5714, -0.0099, -6.1506, -7.2286, -6.2614, -7.8343],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6209, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0839, -5.5719, -0.0098, -6.1511, -7.2292, -6.2619, -7.8349],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6208, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0845, -5.5723, -0.0098, -6.1516, -7.2298, -6.2624, -7.8356],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6207, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0851, -5.5727, -0.0098, -6.1521, -7.2303, -6.2629, -7.8363],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6207, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0856, -5.5732, -0.0098, -6.1526, -7.2309, -6.2634, -7.8370],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6206, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0862, -5.5736, -0.0098, -6.1531, -7.2315, -6.2639, -7.8377],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6206, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0868, -5.5740, -0.0098, -6.1536, -7.2321, -6.2645, -7.8384],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6205, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0874, -5.5745, -0.0098, -6.1541, -7.2327, -6.2650, -7.8391],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6205, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0880, -5.5749, -0.0098, -6.1546, -7.2333, -6.2655, -7.8398],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6204, grad_fn=<AddBackward0>)\n",
      "y_hat tensor([-7.0886, -5.5753, -0.0098, -6.1551, -7.2338, -6.2660, -7.8405],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.6204, grad_fn=<AddBackward0>)\n",
      "Generated explanations in ['edge_mask']\n",
      "Subgraph visualization plot has been saved to 'subgraph.pdf'\n",
      "Weight 320 tensor(0.7320)\n",
      "Weight 321 tensor(0.7320)\n",
      "Weight 1848 tensor(0.7320)\n",
      "Weight 4308 tensor(0.7320)\n",
      "Weight 6014 tensor(0.7322)\n",
      "Weight 7067 tensor(0.7320)\n",
      "Weight 7136 tensor(0.7320)\n",
      "Weight 10304 tensor(0.7322)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=CFExplainer(epochs=20),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "node_index =83\n",
    "print('True value', data.y[node_index])\n",
    "explanation = explainer(data.x, data.edge_index, index=node_index)\n",
    "print(f'Generated explanations in {explanation.available_explanations}')\n",
    "# path = 'feature_importance.png'\n",
    "# explanation.visualize_feature_importance(path, top_k=10)\n",
    "# print(f\"Feature importance plot has been saved to '{path}'\")\n",
    "\n",
    "path = 'subgraph.pdf'\n",
    "explanation.visualize_graph(path)\n",
    "print(f\"Subgraph visualization plot has been saved to '{path}'\")\n",
    "\n",
    "for idx, i in enumerate(explanation['edge_mask']):\n",
    "    if i > 0:\n",
    "        print (\"Weight\", idx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f735b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5606)\n",
      "tensor(0.5632)\n",
      "tensor(0.5487)\n",
      "tensor(0.5514)\n",
      "tensor(0.5621)\n",
      "tensor(0.5524)\n",
      "tensor(0.5479)\n",
      "tensor(0.5542)\n"
     ]
    }
   ],
   "source": [
    "for i in explanation['edge_mask']:\n",
    "    if i > 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cefceafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch_geometric.explain import ExplainerConfig, Explanation, ModelConfig\n",
    "from torch_geometric.explain.algorithm import ExplainerAlgorithm\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import MaskType, ModelMode, ModelTaskLevel\n",
    "\n",
    "\n",
    "class GNNExplainer(ExplainerAlgorithm):\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "        'EPS': 1e-15,\n",
    "    }\n",
    "\n",
    "    def __init__(self, epochs: int = 100, lr: float = 0.01, **kwargs):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.coeffs.update(kwargs)\n",
    "\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ) -> Explanation:\n",
    "        if isinstance(x, dict) or isinstance(edge_index, dict):\n",
    "            raise ValueError(f\"Heterogeneous graphs not yet supported in \"\n",
    "                             f\"'{self.__class__.__name__}'\")\n",
    "\n",
    "        self._train(model, x, edge_index, target=target, index=index, **kwargs)\n",
    "\n",
    "        node_mask = self._post_process_mask(\n",
    "            self.node_mask,\n",
    "            self.hard_node_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "        edge_mask = self._post_process_mask(\n",
    "            self.edge_mask,\n",
    "            self.hard_edge_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "\n",
    "        self._clean_model(model)\n",
    "\n",
    "        return Explanation(node_mask=node_mask, edge_mask=edge_mask)\n",
    "\n",
    "    def supports(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._initialize_masks(x, edge_index)\n",
    "\n",
    "        parameters = []\n",
    "        if self.node_mask is not None:\n",
    "            parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "\n",
    "        optimizer = torch.optim.Adam(parameters, lr=self.lr)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            h = x if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            y_hat, y = model(h, edge_index, **kwargs), target\n",
    "\n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss = self._loss(y_hat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "\n",
    "    def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "        node_mask_type = self.explainer_config.node_mask_type\n",
    "        edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "        device = x.device\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        std = 0.1\n",
    "        if node_mask_type is None:\n",
    "            self.node_mask = None\n",
    "        elif node_mask_type == MaskType.object:\n",
    "            self.node_mask = Parameter(torch.randn(N, 1, device=device) * std)\n",
    "        elif node_mask_type == MaskType.attributes:\n",
    "            self.node_mask = Parameter(torch.randn(N, F, device=device) * std)\n",
    "        elif node_mask_type == MaskType.common_attributes:\n",
    "            self.node_mask = Parameter(torch.randn(1, F, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if edge_mask_type is None:\n",
    "            self.edge_mask = None\n",
    "        elif edge_mask_type == MaskType.object:\n",
    "            std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "            self.edge_mask = Parameter(torch.randn(E, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    def _loss(self, y_hat: Tensor, y: Tensor) -> Tensor:\n",
    "        if self.model_config.mode == ModelMode.binary_classification:\n",
    "            loss = self._loss_binary_classification(y_hat, y)\n",
    "        elif self.model_config.mode == ModelMode.multiclass_classification:\n",
    "            loss = self._loss_multiclass_classification(y_hat, y)\n",
    "        elif self.model_config.mode == ModelMode.regression:\n",
    "            loss = self._loss_regression(y_hat, y)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if self.hard_edge_mask is not None:\n",
    "            assert self.edge_mask is not None\n",
    "            m = self.edge_mask[self.hard_edge_mask].sigmoid()\n",
    "            edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "            loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "            ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "                1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "            loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "        if self.hard_node_mask is not None:\n",
    "            assert self.node_mask is not None\n",
    "            m = self.node_mask[self.hard_node_mask].sigmoid()\n",
    "            node_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "            loss = loss + self.coeffs['node_feat_size'] * node_reduce(m)\n",
    "            ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "                1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "            loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "        print(loss)\n",
    "        return loss\n",
    "\n",
    "    def _clean_model(self, model):\n",
    "        clear_masks(model)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "021d40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch_geometric.explain import ExplainerConfig, Explanation, ModelConfig\n",
    "from torch_geometric.explain.algorithm import ExplainerAlgorithm\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import MaskType, ModelMode, ModelTaskLevel\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "#from torch_geometric.utils import dense_adjacency\n",
    "\n",
    "\n",
    "class CFExplainer(ExplainerAlgorithm):\n",
    "    r\"\"\"The CF-Explainer model from the `\"CF-GNNExplainer: Counterfactual Explanations for Graph Neural\n",
    "Networks\"\n",
    "    <https://arxiv.org/abs/2102.03322>`_ paper for generating CF explanations for GNNs: \n",
    "    the minimal perturbation to the input (graph) data such that the prediction changes.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using :class:`GNNExplainer`, see\n",
    "        `examples/explain/gnn_explainer.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer.py>`_,\n",
    "        `examples/explain/gnn_explainer_ba_shapes.py <https://github.com/\n",
    "        pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        explain/gnn_explainer_ba_shapes.py>`_, and `examples/explain/\n",
    "        gnn_explainer_link_pred.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer_link_pred.py>`_.\n",
    "\n",
    "    Args:\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        **kwargs (optional): Additional hyper-parameters to override default\n",
    "            settings in\n",
    "            :attr:`~torch_geometric.explain.algorithm.GNNExplainer.coeffs`.\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'beta' : .001,\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "        'EPS': 1e-15,\n",
    "    }\n",
    "\n",
    "    def __init__(self, epochs: int = 100, lr: float = 0.01, cf_optimizer = \"SGD\", n_momentum = 0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.cf_optimizer = cf_optimizer\n",
    "        self.n_momentum = n_momentum\n",
    "        self.coeffs.update(kwargs)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n",
    "        self.best_cf_example = None\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ) -> Explanation:\n",
    "        if isinstance(x, dict) or isinstance(edge_index, dict):\n",
    "            raise ValueError(f\"Heterogeneous graphs not yet supported in \"\n",
    "                             f\"'{self.__class__.__name__}'\")\n",
    "\n",
    "        self._train(model, x, edge_index, target=target, index=index, **kwargs)\n",
    "\n",
    "        # node_mask = self._post_process_mask(\n",
    "        #     self.best_cf_example[0],\n",
    "        #     self.hard_node_mask,\n",
    "        #     apply_sigmoid=True,\n",
    "        # )\n",
    "        edge_mask = self._post_process_mask(\n",
    "            self.best_cf_example,\n",
    "            self.hard_edge_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "\n",
    "        self._clean_model(model)\n",
    "\n",
    "        return Explanation(node_mask=node_mask, edge_mask=edge_mask)\n",
    "\n",
    "    def supports(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._initialize_masks(x, edge_index)\n",
    "\n",
    "        parameters = []\n",
    "        if self.node_mask is not None:\n",
    "            parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "    \n",
    "        if self.cf_optimizer == \"SGD\" and self.n_momentum == 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr)\n",
    "        elif self.cf_optimizer == \"SGD\" and self.n_momentum != 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr, nesterov=True, momentum=n_momentum)\n",
    "        elif self.cf_optimizer == \"Adadelta\":\n",
    "            optimizer = torch.optim.Adadelta(parameters, lr=self.lr)\n",
    "        else:\n",
    "            raise Exception(\"Optimizer is not currently supported.\")\n",
    "        \n",
    "        num_cf_examples = 0\n",
    "        original_prediction  = model(x, edge_index, **kwargs)\n",
    "        for i in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            h = x if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask)>=0.5, 1, 0)\n",
    "            set_masks(model, discrete_edge_mask, edge_index, apply_sigmoid=False)\n",
    "            y_hat, y = model(h, edge_index, **kwargs), original_prediction\n",
    "            y_hat_discrete, y_discrete = y_hat.argmax(dim=1), y.argmax(dim=1)\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "\n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss = self._loss(y_hat, y, edge_index)\n",
    "\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss\n",
    "                self.best_cf_example = self.edge_mask\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "\n",
    "    def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "        node_mask_type = self.explainer_config.node_mask_type\n",
    "        edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "        device = x.device\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        std = 0.1\n",
    "        if node_mask_type is None:\n",
    "            self.node_mask = None\n",
    "        elif node_mask_type == MaskType.object:\n",
    "            self.node_mask = Parameter(torch.randn(N, 1, device=device) * std)\n",
    "        elif node_mask_type == MaskType.attributes:\n",
    "            self.node_mask = Parameter(torch.randn(N, F, device=device) * std)\n",
    "        elif node_mask_type == MaskType.common_attributes:\n",
    "            self.node_mask = Parameter(torch.randn(1, F, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if edge_mask_type is None:\n",
    "            self.edge_mask = None\n",
    "        elif edge_mask_type == MaskType.object:\n",
    "            std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "            self.edge_mask = Parameter(torch.randn(E, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    # def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "    #     node_mask_type = self.explainer_config.node_mask_type\n",
    "    #     edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "    #     device = x.device\n",
    "    #     (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "    #     if node_mask_type is None:\n",
    "    #         self.node_mask = None\n",
    "    #     elif node_mask_type == MaskType.object:\n",
    "    #         self.node_mask = Parameter(torch.ones(N, 1, device=device))\n",
    "    #     elif node_mask_type == MaskType.attributes:\n",
    "    #         self.node_mask = Parameter(torch.ones(N, F, device=device))\n",
    "    #     elif node_mask_type == MaskType.common_attributes:\n",
    "    #         self.node_mask = Parameter(torch.ones(1, F, device=device))\n",
    "    #     else:\n",
    "    #         assert False\n",
    "\n",
    "\n",
    "    #     if edge_mask_type is None:\n",
    "    #         self.edge_mask = None\n",
    "    #     elif edge_mask_type == MaskType.object:\n",
    "    #         self.edge_mask = Parameter(torch.ones(E, device=device))\n",
    "    #     else:\n",
    "    #         assert False\n",
    "\n",
    "\n",
    "    def _loss(self, y_hat: Tensor, y: Tensor, edge_index) -> Tensor:\n",
    "        y_hat_discrete = y_hat.argmax(dim=1)\n",
    "        y_discrete = y.argmax(dim=1)\n",
    "\n",
    "        pred_same = (y_hat_discrete == y_discrete).float()\n",
    "        \n",
    "        # if self.model_config.mode == ModelMode.binary_classification:\n",
    "        #     loss = self._loss_binary_classification(y_hat, y)\n",
    "        # elif self.model_config.mode == ModelMode.multiclass_classification:\n",
    "        #     loss = self._loss_multiclass_classification(y_hat, y)\n",
    "        # elif self.model_config.mode == ModelMode.regression:\n",
    "        #     loss = self._loss_regression(y_hat, y)\n",
    "        # else:\n",
    "        #     assert False\n",
    "        # Want negative in front to maximize loss instead of minimizing it to find CFs\n",
    "        discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask)>=0, 1, 0)\n",
    "\n",
    "        loss_pred = - F.nll_loss(y_hat, y_discrete)\n",
    "        adj = dense_adjacency(edge_index, edge_attr=None, num_nodes=None)\n",
    "        discrete_adj = torch.where(torch.sigmoid(self.edge_mask) >= 0, 1, 0)\n",
    "        loss_graph_dist = torch.sum(torch.abs(adj - discrete_adj)) / 2\n",
    "\n",
    "        #loss_graph_dist = sum(sum(abs(to_dense_adj(edge_index) - to_dense_adj(discrete_edge_mask)))) / 2      # Number of edges changed (symmetrical)\n",
    "\n",
    "\t\t# Zero-out loss_pred with pred_same if prediction flips\n",
    "        loss_total = pred_same * loss_pred + self.coeff['beta'] * loss_graph_dist\n",
    "\n",
    "\n",
    "        # if self.hard_edge_mask is not None:\n",
    "        #     assert self.edge_mask is not None\n",
    "        #     m = self.edge_mask[self.hard_edge_mask].sigmoid()\n",
    "        #     edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "        #     loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "        #     ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "        #         1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "        #     loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "        # if self.hard_node_mask is not None:\n",
    "        #     assert self.node_mask is not None\n",
    "        #     m = self.node_mask[self.hard_node_mask].sigmoid()\n",
    "        #     node_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "        #     loss16 = loss + self.coeffs['node_feat_size'] * node_reduce(m)\n",
    "        #     ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "        #         1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "        #     loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "\n",
    "        return loss_total\n",
    "\n",
    "    def _clean_model(self, model):\n",
    "        clear_masks(model)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1377ab4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331a4496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0536b223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960adb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
