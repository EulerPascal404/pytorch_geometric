{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd495b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "#from torch_geometric.explain import CFExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "# Import other necessary modules as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cb5791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.nn import GCNConv, Set2Set\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eb6ad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "dataset = 'cora'\n",
    "path = os.path.join(os.getcwd(), 'data', 'Planetoid')\n",
    "train_dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
    "\n",
    "# Since the dataset is comprised of a single huge graph, we extract that graph by indexing 0.\n",
    "data = train_dataset[0]\n",
    "\n",
    "# Since there is only 1 graph, the train/test split is done by masking regions of the graph. We split the last 500+500 nodes as val and test, and use the rest as the training data.\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.train_mask[:data.num_nodes - 1000] = 1\n",
    "data.val_mask = None\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.test_mask[data.num_nodes - 500:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cefceafc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m node_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      2\u001b[0m x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m----> 3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mGNNExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m node_feat_mask, edge_mask \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_node(node_idx, x, edge_index)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'epochs'"
     ]
    }
   ],
   "source": [
    "node_idx = 10\n",
    "x, edge_index = data.x, data.edge_index\n",
    "explainer = GNNExplainer(model, epochs=200)\n",
    "node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "dim = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(num_features=train_dataset.num_features, dim=dim, num_classes=train_dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index, data), []\n",
    "    for _, mask in data('train_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "loss = 999.0\n",
    "train_acc = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "t = trange(epochs, desc=\"Stats: \", position=0)\n",
    "\n",
    "for epoch in t:\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    log_logits = model(data.x, data.edge_index, data)\n",
    "\n",
    "    # Since the data is a single huge graph, training on the training set is done by masking the nodes that are not in the training set.\n",
    "    loss = F.nll_loss(log_logits[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validate\n",
    "    train_acc, test_acc = test(model, data)\n",
    "    train_loss = loss\n",
    "    \n",
    "    t.set_description('[Train_loss:{:.6f} Train_acc: {:.4f}, Test_acc: {:.4f}]'.format(loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1377ab4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m---> 32\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2689\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2688\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (5) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "# Step 1: Load the benchmark dataset\n",
    "dataset = TUDataset(root='data', name='PROTEINS')\n",
    "data = dataset[0]  # Choose the first graph in the dataset for simplicity\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Step 3: Define a GNN model\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "        self.explainer = GNNExplainer(self.conv2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Step 4: Train the GNN model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNNModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for data in loader:\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Step 5: Explain node importance with GNN-Explainer\n",
    "model.eval()\n",
    "explainer = GNNExplainer(model)\n",
    "node_idx = 0  # Choose the index of the node to explain\n",
    "node_feat_mask = explainer.explain_node(data.to(device), node_idx)\n",
    "\n",
    "# Use the node_feat_mask for visualization or further analysis\n",
    "print(node_feat_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch_geometric.explain import ExplainerConfig, Explanation, ModelConfig\n",
    "from torch_geometric.explain.algorithm import ExplainerAlgorithm\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import MaskType, ModelMode, ModelTaskLevel\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import dense_adjacency\n",
    "\n",
    "\n",
    "class CFExplainer(ExplainerAlgorithm):\n",
    "    r\"\"\"The CF-Explainer model from the `\"CF-GNNExplainer: Counterfactual Explanations for Graph Neural\n",
    "Networks\"\n",
    "    <https://arxiv.org/abs/2102.03322>`_ paper for generating CF explanations for GNNs: \n",
    "    the minimal perturbation to the input (graph) data such that the prediction changes.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using :class:`GNNExplainer`, see\n",
    "        `examples/explain/gnn_explainer.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer.py>`_,\n",
    "        `examples/explain/gnn_explainer_ba_shapes.py <https://github.com/\n",
    "        pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        explain/gnn_explainer_ba_shapes.py>`_, and `examples/explain/\n",
    "        gnn_explainer_link_pred.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer_link_pred.py>`_.\n",
    "\n",
    "    Args:\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        **kwargs (optional): Additional hyper-parameters to override default\n",
    "            settings in\n",
    "            :attr:`~torch_geometric.explain.algorithm.GNNExplainer.coeffs`.\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'beta' : .001,\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "        'EPS': 1e-15,\n",
    "    }\n",
    "\n",
    "    def __init__(self, epochs: int = 100, lr: float = 0.01, cf_optimizer = \"SGD\", n_momentum = 0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.cf_optimizer = cf_optimizer\n",
    "        self.n_momentum = n_momentum\n",
    "        self.coeffs.update(kwargs)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n",
    "        self.best_cf_example = None\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ) -> Explanation:\n",
    "        if isinstance(x, dict) or isinstance(edge_index, dict):\n",
    "            raise ValueError(f\"Heterogeneous graphs not yet supported in \"\n",
    "                             f\"'{self.__class__.__name__}'\")\n",
    "\n",
    "        self._train(model, x, edge_index, target=target, index=index, **kwargs)\n",
    "\n",
    "        # node_mask = self._post_process_mask(\n",
    "        #     self.best_cf_example[0],\n",
    "        #     self.hard_node_mask,\n",
    "        #     apply_sigmoid=True,\n",
    "        # )\n",
    "        edge_mask = self._post_process_mask(\n",
    "            self.best_cf_example,\n",
    "            self.hard_edge_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "\n",
    "        self._clean_model(model)\n",
    "\n",
    "        return Explanation(node_mask=node_mask, edge_mask=edge_mask)\n",
    "\n",
    "    def supports(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._initialize_masks(x, edge_index)\n",
    "\n",
    "        parameters = []\n",
    "        if self.node_mask is not None:\n",
    "            parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "    \n",
    "        if self.cf_optimizer == \"SGD\" and n_momentum == 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr)\n",
    "        elif self.cf_optimizer == \"SGD\" and n_momentum != 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr, nesterov=True, momentum=n_momentum)\n",
    "        elif self.cf_optimizer == \"Adadelta\":\n",
    "            optimizer = torch.optim.Adadelta(parameters, lr=self.lr)\n",
    "        else:\n",
    "            raise Exception(\"Optimizer is not currently supported.\")\n",
    "        \n",
    "        num_cf_examples = 0\n",
    "        original_prediction  = model(x, edge_index, **kwargs)\n",
    "        for i in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            h = x if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask)>=0.5, 1, 0)\n",
    "            set_masks(model, discrete_edge_mask, edge_index, apply_sigmoid=False)\n",
    "            y_hat, y = model(h, edge_index, **kwargs), original_prediction\n",
    "            y_hat_discrete, y_discrete = y_hat.argmax(dim=1), y.argmax(dim=1)\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "\n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss = self._loss(y_hat, y, edge_index)\n",
    "\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss\n",
    "                self.best_cf_example = self.edge_mask\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "\n",
    "    def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "        node_mask_type = self.explainer_config.node_mask_type\n",
    "        edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "        device = x.device\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        std = 0.1\n",
    "        if node_mask_type is None:\n",
    "            self.node_mask = None\n",
    "        elif node_mask_type == MaskType.object:\n",
    "            self.node_mask = Parameter(torch.randn(N, 1, device=device) * std)\n",
    "        elif node_mask_type == MaskType.attributes:\n",
    "            self.node_mask = Parameter(torch.randn(N, F, device=device) * std)\n",
    "        elif node_mask_type == MaskType.common_attributes:\n",
    "            self.node_mask = Parameter(torch.randn(1, F, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if edge_mask_type is None:\n",
    "            self.edge_mask = None\n",
    "        elif edge_mask_type == MaskType.object:\n",
    "            std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "            self.edge_mask = Parameter(torch.randn(E, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    # def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "    #     node_mask_type = self.explainer_config.node_mask_type\n",
    "    #     edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "    #     device = x.device\n",
    "    #     (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "    #     if node_mask_type is None:\n",
    "    #         self.node_mask = None\n",
    "    #     elif node_mask_type == MaskType.object:\n",
    "    #         self.node_mask = Parameter(torch.ones(N, 1, device=device))\n",
    "    #     elif node_mask_type == MaskType.attributes:\n",
    "    #         self.node_mask = Parameter(torch.ones(N, F, device=device))\n",
    "    #     elif node_mask_type == MaskType.common_attributes:\n",
    "    #         self.node_mask = Parameter(torch.ones(1, F, device=device))\n",
    "    #     else:\n",
    "    #         assert False\n",
    "\n",
    "\n",
    "    #     if edge_mask_type is None:\n",
    "    #         self.edge_mask = None\n",
    "    #     elif edge_mask_type == MaskType.object:\n",
    "    #         self.edge_mask = Parameter(torch.ones(E, device=device))\n",
    "    #     else:\n",
    "    #         assert False\n",
    "\n",
    "\n",
    "    def _loss(self, y_hat: Tensor, y: Tensor, edge_index) -> Tensor:\n",
    "        y_hat_discrete = y_hat.argmax(dim=1)\n",
    "        y_discrete = y.argmax(dim=1)\n",
    "\n",
    "        pred_same = (y_hat_discrete == y_discrete).float()\n",
    "        \n",
    "        # if self.model_config.mode == ModelMode.binary_classification:\n",
    "        #     loss = self._loss_binary_classification(y_hat, y)\n",
    "        # elif self.model_config.mode == ModelMode.multiclass_classification:\n",
    "        #     loss = self._loss_multiclass_classification(y_hat, y)\n",
    "        # elif self.model_config.mode == ModelMode.regression:\n",
    "        #     loss = self._loss_regression(y_hat, y)\n",
    "        # else:\n",
    "        #     assert False\n",
    "        # Want negative in front to maximize loss instead of minimizing it to find CFs\n",
    "        discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask)>=0, 1, 0)\n",
    "\n",
    "        loss_pred = - F.nll_loss(y_hat, y_discrete)\n",
    "        adj = dense_adjacency(edge_index, edge_attr=None, num_nodes=None)\n",
    "        discrete_adj = torch.where(torch.sigmoid(self.edge_mask) >= 0, 1, 0)\n",
    "        loss_graph_dist = torch.sum(torch.abs(adj - discrete_adj)) / 2\n",
    "\n",
    "        #loss_graph_dist = sum(sum(abs(to_dense_adj(edge_index) - to_dense_adj(discrete_edge_mask)))) / 2      # Number of edges changed (symmetrical)\n",
    "\n",
    "\t\t# Zero-out loss_pred with pred_same if prediction flips\n",
    "        loss_total = pred_same * loss_pred + self.coeff['beta'] * loss_graph_dist\n",
    "\n",
    "\n",
    "        # if self.hard_edge_mask is not None:\n",
    "        #     assert self.edge_mask is not None\n",
    "        #     m = self.edge_mask[self.hard_edge_mask].sigmoid()\n",
    "        #     edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "        #     loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "        #     ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "        #         1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "        #     loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "        # if self.hard_node_mask is not None:\n",
    "        #     assert self.node_mask is not None\n",
    "        #     m = self.node_mask[self.hard_node_mask].sigmoid()\n",
    "        #     node_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "        #     loss16 = loss + self.coeffs['node_feat_size'] * node_reduce(m)\n",
    "        #     ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "        #         1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "        #     loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "\n",
    "        return loss_total\n",
    "\n",
    "    def _clean_model(self, model):\n",
    "        clear_masks(model)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
