{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd495b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "#from torch_geometric.explain import CFExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "# Import other necessary modules as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40355f86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install jupyterthemes\n",
    "# !jt -t chesterish\n",
    "#!jt -r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb5791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.nn import GCNConv, Set2Set\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead1edc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e75b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a0db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = os.path.join(os.getcwd(), 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, dataset)\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35684884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(data.x, data.edge_index)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8eb6ad07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0196, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0197, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0198, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0198, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0198, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0198, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0198, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0198, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0198, grad_fn=<AddBackward0>)\n",
      "loss_graph_dist tensor(0.)\n",
      "loss_total tensor(-0.0198, grad_fn=<AddBackward0>)\n",
      "Generated explanations in ['edge_mask']\n",
      "Subgraph visualization plot has been saved to 'subgraph.pdf'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=CFExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "node_index = 80\n",
    "explanation = explainer(data.x, data.edge_index, index=node_index)\n",
    "print(f'Generated explanations in {explanation.available_explanations}')\n",
    "\n",
    "# path = 'feature_importance.png'\n",
    "# explanation.visualize_feature_importance(path, top_k=10)\n",
    "# print(f\"Feature importance plot has been saved to '{path}'\")\n",
    "\n",
    "path = 'subgraph.pdf'\n",
    "explanation.visualize_graph(path)\n",
    "print(f\"Subgraph visualization plot has been saved to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cefceafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch_geometric.explain import ExplainerConfig, Explanation, ModelConfig\n",
    "from torch_geometric.explain.algorithm import ExplainerAlgorithm\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import MaskType, ModelMode, ModelTaskLevel\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "#from torch_geometric.utils import dense_adjacency\n",
    "\n",
    "\n",
    "class CFExplainer(ExplainerAlgorithm):\n",
    "    r\"\"\"The CF-Explainer model from the `\"CF-GNNExplainer: Counterfactual Explanations for Graph Neural\n",
    "Networks\"\n",
    "    <https://arxiv.org/abs/2102.03322>`_ paper for generating CF explanations for GNNs: \n",
    "    the minimal perturbation to the input (graph) data such that the prediction changes.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using :class:`GNNExplainer`, see\n",
    "        `examples/explain/gnn_explainer.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer.py>`_,\n",
    "        `examples/explain/gnn_explainer_ba_shapes.py <https://github.com/\n",
    "        pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        explain/gnn_explainer_ba_shapes.py>`_, and `examples/explain/\n",
    "        gnn_explainer_link_pred.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer_link_pred.py>`_.\n",
    "\n",
    "    Args:\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        **kwargs (optional): Additional hyper-parameters to override default\n",
    "            settings in\n",
    "            :attr:`~torch_geometric.explain.algorithm.GNNExplainer.coeffs`.\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'beta' : .001,\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "        'EPS': 1e-15,\n",
    "    }\n",
    "\n",
    "    def __init__(self, epochs: int = 100, lr: float = 0.01, cf_optimizer = \"SGD\", n_momentum = 0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.cf_optimizer = cf_optimizer\n",
    "        self.n_momentum = n_momentum\n",
    "        self.coeffs.update(kwargs)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n",
    "        self.best_cf_example = None\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        index: int = None,\n",
    "        **kwargs,\n",
    "    ) -> Explanation:\n",
    "        if isinstance(x, dict) or isinstance(edge_index, dict):\n",
    "            raise ValueError(f\"Heterogeneous graphs not yet supported in \"\n",
    "                             f\"'{self.__class__.__name__}'\")\n",
    "\n",
    "        self._train(model, x, edge_index, index=index, **kwargs)\n",
    "\n",
    "        # node_mask = self._post_process_mask(\n",
    "        #     self.best_cf_example[0],\n",
    "        #     self.hard_node_mask,\n",
    "        #     apply_sigmoid=True,\n",
    "        # )\n",
    "        node_mask = self._post_process_mask(\n",
    "            self.node_mask,\n",
    "            self.hard_node_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "        edge_mask = self._post_process_mask(\n",
    "            self.edge_mask,\n",
    "            self.hard_edge_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "\n",
    "        self._clean_model(model)\n",
    "\n",
    "        return Explanation(node_mask=node_mask, edge_mask=edge_mask)\n",
    "\n",
    "    def supports(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: int = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._initialize_masks(x, edge_index)\n",
    "        parameters = []\n",
    "        if self.node_mask is not None:\n",
    "            parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "    \n",
    "        if self.cf_optimizer == \"SGD\" and self.n_momentum == 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr)\n",
    "        elif self.cf_optimizer == \"SGD\" and self.n_momentum != 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr, nesterov=True, momentum=n_momentum)\n",
    "        elif self.cf_optimizer == \"Adadelta\":\n",
    "            optimizer = torch.optim.Adadelta(parameters, lr=self.lr)\n",
    "        else:\n",
    "            raise Exception(\"Optimizer is not currently supported.\")\n",
    "        \n",
    "        original_prediction  = model(x, edge_index, **kwargs)\n",
    "        for i in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            h = x # if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            #discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask)>=0.5, 1, 0)\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            y_hat, y = model(h, edge_index, **kwargs), original_prediction\n",
    "            y_hat_discrete, y_discrete = y_hat.argmax(dim=-1), y.argmax(dim=-1)\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "\n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss = self._loss(y_hat, y, edge_index, index=index)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "\n",
    "    def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "        node_mask_type = None\n",
    "        edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "        device = x.device\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        if node_mask_type is None:\n",
    "            self.node_mask = None\n",
    "        elif node_mask_type == MaskType.object:\n",
    "            self.node_mask = Parameter(torch.ones(N, 1, device=device))\n",
    "        elif node_mask_type == MaskType.attributes:\n",
    "            self.node_mask = Parameter(torch.ones(N, F, device=device))\n",
    "        elif node_mask_type == MaskType.common_attributes:\n",
    "            self.node_mask = Parameter(torch.ones(1, F, device=device))\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if edge_mask_type is None:\n",
    "            self.edge_mask = None\n",
    "        elif edge_mask_type == MaskType.object:\n",
    "            self.edge_mask = Parameter(torch.ones(E, device=device))\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    # def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "    #     node_mask_type = self.explainer_config.node_mask_type\n",
    "    #     edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "    #     device = x.device\n",
    "    #     (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "    #     if node_mask_type is None:\n",
    "    #         self.node_mask = None\n",
    "    #     elif node_mask_type == MaskType.object:\n",
    "    #         self.node_mask = Parameter(torch.ones(N, 1, device=device))\n",
    "    #     elif node_mask_type == MaskType.attributes:\n",
    "    #         self.node_mask = Parameter(torch.ones(N, F, device=device))\n",
    "    #     elif node_mask_type == MaskType.common_attributes:\n",
    "    #         self.node_mask = Parameter(torch.ones(1, F, device=device))\n",
    "    #     else:\n",
    "    #         assert False\n",
    "\n",
    "\n",
    "    #     if edge_mask_type is None:\n",
    "    #         self.edge_mask = None\n",
    "    #     elif edge_mask_type == MaskType.object:\n",
    "    #         self.edge_mask = Parameter(torch.ones(E, device=device))\n",
    "    #     else:\n",
    "    #         assert False\n",
    "\n",
    "\n",
    "    def _loss(self, y_hat: Tensor, y: Tensor, edge_index, index) -> Tensor:\n",
    "        y_hat_discrete = torch.argmax(y_hat, dim=-1)  # Compute argmax along the class dimension\n",
    "        y_discrete = torch.argmax(y, dim=-1)  # Compute argmax along the class dimension\n",
    "\n",
    "        pred_same = (y_hat_discrete == y_discrete).float()\n",
    "        if self.model_config.mode == ModelMode.binary_classification:\n",
    "            loss_pred = - self._loss_binary_classification(y_hat, y.long())\n",
    "        elif self.model_config.mode == ModelMode.multiclass_classification:\n",
    "            loss_pred = - self._loss_multiclass_classification(y_hat, y.long())\n",
    "#         elif self.model_config.mode == ModelMode.regression:\n",
    "#             loss_pred = - self._loss_regression(y_hat, y)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask) > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
    "        new_edge_index = edge_index * discrete_edge_mask\n",
    "\n",
    "        loss_graph_dist = torch.sum(torch.abs(new_edge_index - edge_index)) / 2\n",
    "        print(\"loss_graph_dist\", loss_graph_dist)\n",
    "        loss_total = pred_same * loss_pred + 10000* self.coeffs['beta'] * loss_graph_dist\n",
    "        print(\"loss_total\", loss_total)\n",
    "\n",
    "        return loss_total\n",
    "\n",
    "    def _clean_model(self, model):\n",
    "        clear_masks(model)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch_geometric.explain import ExplainerConfig, Explanation, ModelConfig\n",
    "from torch_geometric.explain.algorithm import ExplainerAlgorithm\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import MaskType, ModelMode, ModelTaskLevel\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "#from torch_geometric.utils import dense_adjacency\n",
    "\n",
    "\n",
    "class CFExplainer(ExplainerAlgorithm):\n",
    "    r\"\"\"The CF-Explainer model from the `\"CF-GNNExplainer: Counterfactual Explanations for Graph Neural\n",
    "Networks\"\n",
    "    <https://arxiv.org/abs/2102.03322>`_ paper for generating CF explanations for GNNs: \n",
    "    the minimal perturbation to the input (graph) data such that the prediction changes.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using :class:`GNNExplainer`, see\n",
    "        `examples/explain/gnn_explainer.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer.py>`_,\n",
    "        `examples/explain/gnn_explainer_ba_shapes.py <https://github.com/\n",
    "        pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        explain/gnn_explainer_ba_shapes.py>`_, and `examples/explain/\n",
    "        gnn_explainer_link_pred.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/explain/gnn_explainer_link_pred.py>`_.\n",
    "\n",
    "    Args:\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        **kwargs (optional): Additional hyper-parameters to override default\n",
    "            settings in\n",
    "            :attr:`~torch_geometric.explain.algorithm.GNNExplainer.coeffs`.\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'beta' : .001,\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "        'EPS': 1e-15,\n",
    "    }\n",
    "\n",
    "    def __init__(self, epochs: int = 100, lr: float = 0.01, cf_optimizer = \"SGD\", n_momentum = 0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.cf_optimizer = cf_optimizer\n",
    "        self.n_momentum = n_momentum\n",
    "        self.coeffs.update(kwargs)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n",
    "        self.best_cf_example = None\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ) -> Explanation:\n",
    "        if isinstance(x, dict) or isinstance(edge_index, dict):\n",
    "            raise ValueError(f\"Heterogeneous graphs not yet supported in \"\n",
    "                             f\"'{self.__class__.__name__}'\")\n",
    "\n",
    "        self._train(model, x, edge_index, target=target, index=index, **kwargs)\n",
    "\n",
    "        # node_mask = self._post_process_mask(\n",
    "        #     self.best_cf_example[0],\n",
    "        #     self.hard_node_mask,\n",
    "        #     apply_sigmoid=True,\n",
    "        # )\n",
    "        edge_mask = self._post_process_mask(\n",
    "            self.best_cf_example,\n",
    "            self.hard_edge_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "\n",
    "        self._clean_model(model)\n",
    "\n",
    "        return Explanation(node_mask=node_mask, edge_mask=edge_mask)\n",
    "\n",
    "    def supports(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._initialize_masks(x, edge_index)\n",
    "\n",
    "        parameters = []\n",
    "        if self.node_mask is not None:\n",
    "            parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "    \n",
    "        if self.cf_optimizer == \"SGD\" and self.n_momentum == 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr)\n",
    "        elif self.cf_optimizer == \"SGD\" and self.n_momentum != 0.0:\n",
    "            optimizer = torch.optim.SGD(parameters, lr=self.lr, nesterov=True, momentum=n_momentum)\n",
    "        elif self.cf_optimizer == \"Adadelta\":\n",
    "            optimizer = torch.optim.Adadelta(parameters, lr=self.lr)\n",
    "        else:\n",
    "            raise Exception(\"Optimizer is not currently supported.\")\n",
    "        \n",
    "        num_cf_examples = 0\n",
    "        original_prediction  = model(x, edge_index, **kwargs)\n",
    "        for i in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            h = x if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask)>=0.5, 1, 0)\n",
    "            set_masks(model, discrete_edge_mask, edge_index, apply_sigmoid=False)\n",
    "            y_hat, y = model(h, edge_index, **kwargs), original_prediction\n",
    "            y_hat_discrete, y_discrete = y_hat.argmax(dim=1), y.argmax(dim=1)\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "\n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss = self._loss(y_hat, y, edge_index)\n",
    "\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss\n",
    "                self.best_cf_example = self.edge_mask\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "\n",
    "    def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "        node_mask_type = self.explainer_config.node_mask_type\n",
    "        edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "        device = x.device\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        std = 0.1\n",
    "        if node_mask_type is None:\n",
    "            self.node_mask = None\n",
    "        elif node_mask_type == MaskType.object:\n",
    "            self.node_mask = Parameter(torch.randn(N, 1, device=device) * std)\n",
    "        elif node_mask_type == MaskType.attributes:\n",
    "            self.node_mask = Parameter(torch.randn(N, F, device=device) * std)\n",
    "        elif node_mask_type == MaskType.common_attributes:\n",
    "            self.node_mask = Parameter(torch.randn(1, F, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if edge_mask_type is None:\n",
    "            self.edge_mask = None\n",
    "        elif edge_mask_type == MaskType.object:\n",
    "            std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "            self.edge_mask = Parameter(torch.randn(E, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    # def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "    #     node_mask_type = self.explainer_config.node_mask_type\n",
    "    #     edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "    #     device = x.device\n",
    "    #     (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "    #     if node_mask_type is None:\n",
    "    #         self.node_mask = None\n",
    "    #     elif node_mask_type == MaskType.object:\n",
    "    #         self.node_mask = Parameter(torch.ones(N, 1, device=device))\n",
    "    #     elif node_mask_type == MaskType.attributes:\n",
    "    #         self.node_mask = Parameter(torch.ones(N, F, device=device))\n",
    "    #     elif node_mask_type == MaskType.common_attributes:\n",
    "    #         self.node_mask = Parameter(torch.ones(1, F, device=device))\n",
    "    #     else:\n",
    "    #         assert False\n",
    "\n",
    "\n",
    "    #     if edge_mask_type is None:\n",
    "    #         self.edge_mask = None\n",
    "    #     elif edge_mask_type == MaskType.object:\n",
    "    #         self.edge_mask = Parameter(torch.ones(E, device=device))\n",
    "    #     else:\n",
    "    #         assert False\n",
    "\n",
    "\n",
    "    def _loss(self, y_hat: Tensor, y: Tensor, edge_index) -> Tensor:\n",
    "        y_hat_discrete = y_hat.argmax(dim=1)\n",
    "        y_discrete = y.argmax(dim=1)\n",
    "\n",
    "        pred_same = (y_hat_discrete == y_discrete).float()\n",
    "        \n",
    "        # if self.model_config.mode == ModelMode.binary_classification:\n",
    "        #     loss = self._loss_binary_classification(y_hat, y)\n",
    "        # elif self.model_config.mode == ModelMode.multiclass_classification:\n",
    "        #     loss = self._loss_multiclass_classification(y_hat, y)\n",
    "        # elif self.model_config.mode == ModelMode.regression:\n",
    "        #     loss = self._loss_regression(y_hat, y)\n",
    "        # else:\n",
    "        #     assert False\n",
    "        # Want negative in front to maximize loss instead of minimizing it to find CFs\n",
    "        discrete_edge_mask = torch.where(torch.sigmoid(self.edge_mask)>=0, 1, 0)\n",
    "\n",
    "        loss_pred = - F.nll_loss(y_hat, y_discrete)\n",
    "        adj = dense_adjacency(edge_index, edge_attr=None, num_nodes=None)\n",
    "        discrete_adj = torch.where(torch.sigmoid(self.edge_mask) >= 0, 1, 0)\n",
    "        loss_graph_dist = torch.sum(torch.abs(adj - discrete_adj)) / 2\n",
    "\n",
    "        #loss_graph_dist = sum(sum(abs(to_dense_adj(edge_index) - to_dense_adj(discrete_edge_mask)))) / 2      # Number of edges changed (symmetrical)\n",
    "\n",
    "\t\t# Zero-out loss_pred with pred_same if prediction flips\n",
    "        loss_total = pred_same * loss_pred + self.coeff['beta'] * loss_graph_dist\n",
    "\n",
    "\n",
    "        # if self.hard_edge_mask is not None:\n",
    "        #     assert self.edge_mask is not None\n",
    "        #     m = self.edge_mask[self.hard_edge_mask].sigmoid()\n",
    "        #     edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "        #     loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "        #     ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "        #         1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "        #     loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "        # if self.hard_node_mask is not None:\n",
    "        #     assert self.node_mask is not None\n",
    "        #     m = self.node_mask[self.hard_node_mask].sigmoid()\n",
    "        #     node_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "        #     loss16 = loss + self.coeffs['node_feat_size'] * node_reduce(m)\n",
    "        #     ent = -m * torch.log(m + self.coeffs['EPS']) - (\n",
    "        #         1 - m) * torch.log(1 - m + self.coeffs['EPS'])\n",
    "        #     loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "\n",
    "        return loss_total\n",
    "\n",
    "    def _clean_model(self, model):\n",
    "        clear_masks(model)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1377ab4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331a4496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0536b223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960adb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
